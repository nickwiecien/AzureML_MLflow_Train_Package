{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f11c844",
   "metadata": {},
   "source": [
    "## Azure Machine Learning - Model Training Pipeline\n",
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to load data from an AML-linked blob storage account, split the data into testing and training subsets, train a classification model, evaluate and register the model, and then package the model into a Docker container and push to a container registry. For the final evaluation step a champion vs. challenger A/B test is performed using a target metric of interest so that the best performing model is always reflected in the model registry.\n",
    "\n",
    "Note: This notebook builds from the Iris Setosa sample dataset available in Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23cb83",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf498ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf95d1",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastores\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore.\n",
    "\n",
    "<i>Note:</i> For execution in MLOps CI/CD pipelines, some of the hard-coded values below can be parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = None\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "except Exception:\n",
    "    ws = Workspace(subscription_id=os.getenv('SUBSCRIPTION_ID'),  resource_group = os.getenv('RESOURCE_GROUP'), workspace_name = os.getenv('WORKSPACE_NAME'))\n",
    "\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cluster002'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19a41d",
   "metadata": {},
   "source": [
    "### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py).\n",
    "\n",
    "Here, we also register the environment to the AML workspace so that it can be used for future retraining and inferencing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = Environment(name='sample_env')\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.docker.arguments = ['-v', '/var/run/docker.sock:/var/run/docker.sock']\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create()\n",
    "run_config.environment.python.conda_dependencies.set_pip_requirements([\n",
    "    'requests==2.26.0',\n",
    "    'pandas==0.25.3',\n",
    "    'numpy==1.19.2',\n",
    "    'scikit-learn==0.22.1',\n",
    "    'joblib==0.14.1',\n",
    "    'azureml-defaults==1.43.0',\n",
    "    'azureml-mlflow==1.43.0',\n",
    "    'mlflow==1.28.0',\n",
    "    'scipy==1.5.3',\n",
    "    'docker==5.0.3'\n",
    "])\n",
    "run_config.environment.python.conda_dependencies.set_python_version('3.8.10')\n",
    "\n",
    "#Register environment for reuse \n",
    "run_config.environment.register(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306c27b",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = OutputFileDatasetConfig(name='Raw_Data', destination=(default_ds, 'raw_data/{run-id}')).read_delimited_files().register_on_complete(name='Raw_Data')\n",
    "training_data = OutputFileDatasetConfig(name='Training_Data', destination=(default_ds, 'training_data/{run-id}')).read_delimited_files().register_on_complete(name='Training_Data')\n",
    "testing_data = OutputFileDatasetConfig(name='Testing_Data', destination=(default_ds, 'testing_data/{run-id}')).read_delimited_files().register_on_complete(name='Testing_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df954c13",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we specify the percent of data (0.0-1.0) that should be added to our testing dataset, along with the target column name, and pass these as variable parameters into the pipeline at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ff964",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_size = PipelineParameter(name='testing_size', default_value=0.3)\n",
    "target_column = PipelineParameter(name='target_column', default_value='target')\n",
    "model_name = PipelineParameter(name='model_name', default_value='iris-classification')\n",
    "model_description = PipelineParameter(name='model_description', default_value='Scikit-Learn K-Neighbors Classifier for Iris Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19742a8b",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of five distinct steps all of which execute an associated python script located in the ./pipeline_script_steps dir. First, we call get_data.py and retrieve data from the registered blob datastore and register this dataset as Raw_Data. From here we run split_data.py which splits the raw data into test and train datasets according to the variable `testing_size` parameter - both of which are subsequently registered. Then, we pass the test and training datasets into a step that runs train_model.py which trains the iris classifier and computes and registers a set of metrics. Afterwards, the final step executes evaluate_and_register.py which loads both the new model (challenger) and current best model (champion) into code and evaluates the provided test dataset. Based on the `accuracy` metric, if the challenger model performs better, or no model has been registered to-date, the model is registered in the workspace. Finally, to support deployments to different environments, we package the challenger model (if it is the strongest performer) into a docker container and add it to the AML linked container regsitry.\n",
    "\n",
    "<i>Note:</i> The first step `add_data_step` is included purely for demonstration purposes. This step serves to move data into an attached blob storage location to be consumed in downstream steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step for demo: Seed datastore with\n",
    "# Iris Setosa dataset\n",
    "add_data_step = PythonScriptStep(\n",
    "    name='Add Sample Data to Blob Storage',\n",
    "    script_name='add_data.py',\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "# Get raw data from AML-linked datastore\n",
    "# Register tabular dataset after retrieval\n",
    "get_data_step = PythonScriptStep(\n",
    "    name='Get Data from Blob Storage',\n",
    "    script_name='get_data.py',\n",
    "    arguments =['--raw_data', raw_data],\n",
    "    outputs=[raw_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "get_data_step.run_after(add_data_step)\n",
    "\n",
    "# Load raw data and split into test and train\n",
    "# datasets according to the specified split percentage\n",
    "split_data_step = PythonScriptStep(\n",
    "    name='Split Train and Test Data',\n",
    "    script_name='split_data.py',\n",
    "    arguments =['--training_data', training_data,\n",
    "                '--testing_data', testing_data,\n",
    "                '--testing_size', testing_size],\n",
    "    inputs=[raw_data.as_input(name='Raw_Data')],\n",
    "    outputs=[training_data, testing_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "# Train iris classification model using split\n",
    "# test and train datasets. Both the scaler and trained model\n",
    "# will be saved as PipelineData\n",
    "train_model_step = PythonScriptStep(\n",
    "    name='Train Model',\n",
    "    script_name='train_model.py',\n",
    "    arguments =[\n",
    "                '--target_column', target_column\n",
    "    ],\n",
    "    inputs=[training_data.as_input(name='Training_Data'),\n",
    "            testing_data.as_input(name='Testing_Data')\n",
    "           ],\n",
    "    outputs=[],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "#Evaluate and register model here\n",
    "#Compare metrics from current model and register if better than current\n",
    "#best model\n",
    "evaluate_and_register_step = PythonScriptStep(\n",
    "    name='Evaluate and Register Model',\n",
    "    script_name='evaluate_and_register.py',\n",
    "    arguments=[\n",
    "               '--target_column', target_column,\n",
    "               '--model_name', model_name,\n",
    "               '--model_description', model_description],\n",
    "    inputs=[training_data.as_input(name='Training_Data'),\n",
    "            testing_data.as_input(name='Testing_Data')],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "evaluate_and_register_step.run_after(train_model_step)\n",
    "\n",
    "#Package model step\n",
    "#Container registered champion model here for deployment to target\n",
    "#endpoints\n",
    "package_model_step = PythonScriptStep(\n",
    "    name='Package Model',\n",
    "    script_name='package_model.py',\n",
    "    arguments=[\n",
    "               '--model_name', model_name\n",
    "    ],\n",
    "    inputs=[testing_data.as_input(name='Testing_Data')],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "package_model_step.run_after(evaluate_and_register_step)\n",
    "\n",
    "# Alternate step for checking connectivity to ACR\n",
    "# Note: assumes a version '1' of the target model has been added to the container registry\n",
    "copy_model_step = PythonScriptStep(\n",
    "    name='Copy Model',\n",
    "    script_name='copy_model_image.py',\n",
    "    arguments=['--model_name', model_name, '--model_version', '1'],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "copy_model_step.run_after(evaluate_and_register_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33ec2b",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af25abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original pipeline with model package creation\n",
    "# pipeline = Pipeline(workspace=ws, steps=[add_data_step, get_data_step, split_data_step, train_model_step, evaluate_and_register_step, package_model_step])\n",
    "\n",
    "# Pipeline with ONLY copy_model_step\n",
    "#pipeline = Pipeline(workspace=ws, steps=[copy_model_step])\n",
    "\n",
    "# Updated pipeline with with copy model in lieu of model package creation step\n",
    "pipeline = Pipeline(workspace=ws, steps=[add_data_step, get_data_step, split_data_step, train_model_step, evaluate_and_register_step, copy_model_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecc112",
   "metadata": {},
   "source": [
    "### Optional: Trigger a Pipeline Execution from the Notebook\n",
    "You can create an Experiment (logical collection for runs) and submit a pipeline run directly from this notebook by running the commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9400986",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = os.getenv('EXPERIMENT_NAME', 'sample-training-pipeline-run')\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88627d9a",
   "metadata": {},
   "source": [
    "### Create a Published PipelineEndpoint\n",
    "Once we have created our pipeline we will look to retrain our model periodically as new data becomes available. By publishing our pipeline to a `PipelineEndpoint` we can iterate on our pipeline definition but maintain a consistent REST API endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bea386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "# def published_pipeline_to_pipeline_endpoint(\n",
    "#     workspace,\n",
    "#     published_pipeline,\n",
    "#     pipeline_endpoint_name,\n",
    "#     pipeline_endpoint_description=\"Endpoint to my pipeline\",\n",
    "# ):\n",
    "#     try:\n",
    "#         pipeline_endpoint = PipelineEndpoint.get(\n",
    "#             workspace=workspace, name=pipeline_endpoint_name\n",
    "#         )\n",
    "#         print(\"using existing PipelineEndpoint...\")\n",
    "#         pipeline_endpoint.add_default(published_pipeline)\n",
    "#     except Exception as ex:\n",
    "#         print(ex)\n",
    "#         # create PipelineEndpoint if it doesn't exist\n",
    "#         print(\"PipelineEndpoint does not exist, creating one for you...\")\n",
    "#         pipeline_endpoint = PipelineEndpoint.publish(\n",
    "#             workspace=workspace,\n",
    "#             name=pipeline_endpoint_name,\n",
    "#             pipeline=published_pipeline,\n",
    "#             description=pipeline_endpoint_description\n",
    "#         )\n",
    "\n",
    "\n",
    "# pipeline_endpoint_name = 'Classification Model Training Pipeline'\n",
    "# pipeline_endpoint_description = 'Sample pipeline for training, evaluating, and registering a classification model based on the Iris Setosa dataset'\n",
    "\n",
    "# published_pipeline = pipeline.publish(name=pipeline_endpoint_name,\n",
    "#                                      description=pipeline_endpoint_description,\n",
    "#                                      continue_on_step_failure=False)\n",
    "\n",
    "# published_pipeline_to_pipeline_endpoint(\n",
    "#     workspace=ws,\n",
    "#     published_pipeline=published_pipeline,\n",
    "#     pipeline_endpoint_name=pipeline_endpoint_name,\n",
    "#     pipeline_endpoint_description=pipeline_endpoint_description\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
